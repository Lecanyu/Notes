<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Human Pose Estimation</title>
  <meta name="description" content="Personal notes (memorandum).">


  <link rel="stylesheet" href="/Notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/Notes/paper/human_pose/">
  <link rel="alternate" type="application/rss+xml" title="Notes" href="http://localhost:4000/Notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/Notes/">Contents</a>
		<a href="https://github.com/Lecanyu/Notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Human pose estimation</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>There are some popular paper about human pose estimation.
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">There is a nice human pose estimation overview in <a href="https://zhuanlan.zhihu.com/p/85506259">here</a>.  </span>
I briefly introduce them here so that I can review quickly in the future.</p>

<p>This is not an exhausted list and I will update it constantly.</p>

<h2 id="concept-overview">Concept overview</h2>
<p>Single human pose estimation: estimate body keypoints in a single person.</p>

<p>Multiple human pose estimation: estimate body keypoints in mutiple persons.</p>

<p>Single human pose estimation can be seen a special case of multiple estimations. 
Current research interest is in deep learning based multiple human pose estimation (previously, there are some traditional methods which is based on hand-crafted features and graphical models).
To solve it, there are two ways:</p>

<p>1.top-down menthod which detects and crops single person one by one and then applies single human pose estimation method.</p>

<p>2.Button-up method which detects all possible keypoints regardless which person they belong and then solve the association problem (i.e. associate those keypoints with person instance).</p>

<p>Since Top-down methods take advantage of object detection results as the prior, it is usually more accurate but kind of slow.
Whereas buttom-up methods are more unified (without person detection) and fast with the cost of accuracy.</p>

<p>Something more:</p>

<p>In deep learning based human pose estimation, researchers previously focused on image context, but now they gradually realize that the resolution of image and feature map is crucial in detection performance. 
Because the keypoint detection is a pixel-level detection which is fine-grained. 
To solve it, a trivial approach is to directly resize original image to a larger resolution.
And there is a recent surge of interest in network design for high resolution feature representation.
The <a href="https://arxiv.org/pdf/1908.07919.pdf">paper</a> (Deep high-resolution representation learning for visual recognition) discusses this aspect.</p>

<h2 id="cpm">CPM</h2>

<p>Original paper ‘‘Convolutional Pose Machines’’. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Original CPM paper see <a href="https://arxiv.org/pdf/1602.00134.pdf">here</a>.  </span></p>

<p>Key features:</p>

<p>1.CPM is a single human pose estimation method.</p>

<p>2.It designs a sequential composition of convolutional architectures which model the spatial relationship via large receptive field. 
Besides, it enforces intermediate supervision (i.e. loss optimization) to relieve gradient vanish problems.</p>

<figure><figcaption></figcaption><img src="/Notes/assets/paper/human_pose_estimation/CPM1.png" /></figure>

<p>3.Instead of regressing keypoint locations, it introduces heatmap as the training target.
Heatmap has several advantages:
(1) it introduces spatial uncertainty.
(2) Unlike 1d location in regression, heatmap is 2d representation which implicitly encodes spatial information.
As a result, the heatmap can be fed into following convolutional stages as an input.</p>

<p>Note that training dataset only provide the keypoint location groundtruth, which can be trained as target.
The authors apply Gauss kernal on groundtruth points to generate the groundtruth heatmap.</p>

<h2 id="hourglass">Hourglass</h2>

<p>Original paper ‘‘Stacked Hourglass Networks for Human Pose Estimation’’. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Original Hourglass paper see <a href="https://arxiv.org/pdf/1603.06937.pdf">here</a>.  </span></p>

<p>Key features:</p>

<p>1.It is a single person estimation method.</p>

<p>2.This paper design a novel network architecture, called stacked hourglass. 
The authors claim this architecture can capture local and global features better.</p>

<h2 id="associative-embedding">Associative Embedding</h2>

<p>Original paper ‘‘Associative Embedding: End-to-End Learning for Joint Detection and Grouping’’. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Original Associative Embedding paper see <a href="https://arxiv.org/pdf/1611.05424.pdf">here</a>.  </span></p>

<p>Key features:</p>

<p>1.It is a multiple person estimation method.</p>

<p>2.It estimates all limb keypoints and associative embedding simultaneously. 
Then, it uses the associative embedding values to group those keypoints.</p>

<figure><figcaption></figcaption><img src="/Notes/assets/paper/human_pose_estimation/associative_embedding1.png" /></figure>

<p>3.It uses the stacked hourglass as the network arthitecture and designs new associative embedding loss which enforces the keypoints in the same person have similar embedding values.
The loss is described as below:</p>

<div class="mathblock"><script type="math/tex; mode=display">
\bar h_n = \frac{1}{K} \sum_k h_k(x_{nk})
</script></div>
<p>where <script type="math/tex">h_k \in R^{W\times H}</script> is the predicted embedding value heatmap for <script type="math/tex">k</script>-th body joint (total <script type="math/tex">K</script> body joints).
<script type="math/tex">h(x)</script> is the embedding value at location <script type="math/tex">x</script>.
<script type="math/tex">x_{nk}</script> is the groundtruth location of the <script type="math/tex">n</script>-th person’s <script type="math/tex">k</script>-th body joint.</p>

<p>So the equation above calculates the mean embedding value of all <script type="math/tex">K</script> body joints at groundtruth location for <script type="math/tex">n</script>-th person (total <script type="math/tex">N</script> person in an image).</p>

<p>The embedding associative loss is defined as</p>
<div class="mathblock"><script type="math/tex; mode=display">
L_g(h, T) = \frac{1}{N} \sum_n \sum_k (\bar h_n - h_k(x_{nk}))^2 + \frac{1}{N^2} \sum_n \sum_{n^{'}} \exp\{-\frac{ (\bar h_n - \bar h_{n^{'}})^2 }{2\sigma^2}\}
</script></div>
<p>The first term means that all body joints in one person should be closed to <script type="math/tex">\bar h_n</script>.
The second term means that the mean embedding value of different person should be different (large difference gives small loss).</p>

<h2 id="openpose">OpenPose</h2>

<p>Original paper ‘‘OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields’’. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Original OpenPose paper see <a href="https://arxiv.org/pdf/1812.08008.pdf">here</a>.  </span></p>

<p>I just quickly browse this paper without careful reading. 
If need, I’ll read this paper again.</p>

<p>Key features:</p>

<p>1.This paper is a buttom-up method: using CPM (convolution pose machine architecture) as the backbone to predict keypoint heatmap and keypoint association simultaneously.</p>

<p>2.It proposes part affinity field (PAF) to represent keypoint association.
PAF is a vector field which indicates the keypoint connected direction. 
The vector (i.e. connected direction) will be treated as score in solving bipartite matching.</p>

<figure><figcaption></figcaption><img src="/Notes/assets/paper/human_pose_estimation/openpose1.png" /></figure>

<h2 id="hrnet">HRNet</h2>

<p>Original paper ‘‘Deep High-Resolution Representation Learning for Human Pose Estimation’’. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Original HRNet paper see <a href="https://arxiv.org/pdf/1902.09212.pdf">here</a>.  </span></p>

<p>Key features:</p>

<p>1.Researchers realize that the image or feature map resolution is crucial in keypoint detection performance.
So the author design a new network architecture for obtaining high resolution feature representation.</p>

<figure><figcaption></figcaption><img src="/Notes/assets/paper/human_pose_estimation/HRNet1.png" /></figure>

<p>2.In original paper, this method is a top-down method, but it should be easily modified for buttom-up estimation like associative embedding.</p>

<p>3.This paper still enforces intermediate loss optimization.</p>

<h2 id="hhrnet">HHRNet</h2>

<p>Original paper ‘‘HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation’’. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Original HHRNet paper see <a href="https://arxiv.org/pdf/1908.10357.pdf">here</a>.  </span></p>

<p>Key features:</p>

<p>1.This paper is mainly based on HRNet. It propose a higher resolution of feature representation via deconvolution operation.</p>

<figure><figcaption></figcaption><img src="/Notes/assets/paper/human_pose_estimation/HHRNet1.png" /></figure>
<p>The red box in figure is their contribution.</p>

<p>2.This method demonstrates the buttom-up approach with higher resolution of feature representation.
It adopts associative embedding strategy for keypoints grouping.</p>




    </article>
    <span class="print-footer">Human Pose Estimation - Canyu Le</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2020 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;CANYU LE &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2020</span> 
</div>  
</footer>

  </body>
</html>
