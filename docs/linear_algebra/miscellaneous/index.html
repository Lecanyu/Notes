<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Miscellaneous (linear algebra)</title>
  <meta name="description" content="Personal notes (memorandum).">


  <link rel="stylesheet" href="/Notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/Notes/linear_algebra/miscellaneous/">
  <link rel="alternate" type="application/rss+xml" title="Notes" href="http://localhost:4000/Notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/Notes/">Contents</a>
		<a href="https://lecanyu.github.io/">Resume</a>
		<a href="https://github.com/Lecanyu/Notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Miscellaneous (linear algebra)</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<h2 id="singular-value-decomposition-svd"><strong><em>Singular value decomposition (SVD)</em></strong></h2>
<div class="mathblock"><script type="math/tex; mode=display">
A = U \Sigma V^T
</script></div>
<p>where <script type="math/tex">A</script> can be an arbitrary matrix (letâ€™s say n x m). Then <script type="math/tex">U</script> is a n x n matrix, <script type="math/tex">\Sigma</script> is a n x m matrix, and <script type="math/tex">V</script> is a m x m matrix.</p>

<p>The column vectors in <script type="math/tex">U</script> are mutually orthonormal, and the row vectors in <script type="math/tex">V</script> are mutually orthonormal.</p>

<p>If we pick only top-<script type="math/tex">k</script> big singular value, then we have <script type="math/tex">A \approx U_{n \times k} \Sigma_{k \times k} V_{k \times m}</script>.</p>

<p>Here is a SVD example by using Eigen library.</p>

<figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"> 
<span class="cp">#include &lt;iostream&gt;
#include &lt;Eigen/Eigen&gt;
</span>
<span class="k">using</span> <span class="k">namespace</span> <span class="n">Eigen</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="k">typedef</span> <span class="n">Matrix</span><span class="o">&lt;</span><span class="kt">double</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="o">&gt;</span> <span class="n">Matrix2x4</span><span class="p">;</span>
    <span class="n">Matrix2x4</span> <span class="n">A</span><span class="p">;</span>
    <span class="n">A</span><span class="o">&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">;</span>

    <span class="c1">// A = U * S * V^T</span>
    <span class="n">JacobiSVD</span><span class="o">&lt;</span><span class="n">Matrix2x4</span><span class="o">&gt;</span> <span class="n">svd</span><span class="p">(</span> <span class="n">A</span><span class="p">,</span> <span class="n">ComputeThinU</span> <span class="o">|</span> <span class="n">ComputeThinV</span><span class="p">);</span>      
    <span class="k">auto</span> <span class="n">U</span> <span class="o">=</span> <span class="n">svd</span><span class="p">.</span><span class="n">matrixU</span><span class="p">();</span>          <span class="c1">// left matrix 2x2</span>
    <span class="k">auto</span> <span class="n">V</span> <span class="o">=</span> <span class="n">svd</span><span class="p">.</span><span class="n">matrixV</span><span class="p">();</span>          <span class="c1">// right matrix 4x4</span>
    <span class="k">auto</span> <span class="n">S</span> <span class="o">=</span> <span class="n">svd</span><span class="p">.</span><span class="n">singularValues</span><span class="p">();</span>   <span class="c1">// column vector, singular values</span>
    <span class="n">MatrixXd</span> <span class="n">S_mat</span> <span class="o">=</span> <span class="n">MatrixXd</span><span class="o">::</span><span class="n">Zero</span><span class="p">(</span><span class="n">U</span><span class="p">.</span><span class="n">cols</span><span class="p">(),</span> <span class="n">V</span><span class="p">.</span><span class="n">cols</span><span class="p">());</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">S_mat</span><span class="p">.</span><span class="n">rows</span><span class="p">(),</span> <span class="n">S_mat</span><span class="p">.</span><span class="n">cols</span><span class="p">());</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
        <span class="n">S_mat</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">S</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"U:</span><span class="se">\n</span><span class="s">"</span><span class="o">&lt;&lt;</span><span class="n">U</span><span class="o">&lt;&lt;</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"sigma:</span><span class="se">\n</span><span class="s">"</span><span class="o">&lt;&lt;</span> <span class="n">S_mat</span> <span class="o">&lt;&lt;</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"V:</span><span class="se">\n</span><span class="s">"</span><span class="o">&lt;&lt;</span><span class="n">V</span><span class="o">&lt;&lt;</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"original matrix:</span><span class="se">\n</span><span class="s">"</span><span class="o">&lt;&lt;</span> <span class="n">U</span> <span class="o">*</span> <span class="n">S_mat</span> <span class="o">*</span> <span class="n">V</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">&lt;&lt;</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<h2 id="principle-component-analysis-pca"><strong><em>Principle component analysis (PCA)</em></strong></h2>
<div class="mathblock"><script type="math/tex; mode=display">
A = V \Sigma V^T
</script></div>

<p>where <script type="math/tex">A</script> usually is a covariance matrix which is a symmetrical matrix. <script type="math/tex">V</script> is composed by a series of eigen vectors which are mutually orthonormal. <script type="math/tex">\Sigma</script> is corresponding eigen value.</p>

<h2 id="covariance-matrix"><strong><em>Covariance matrix</em></strong></h2>
<p>Variance measure the concentration property within a series data</p>
<div class="mathblock"><script type="math/tex; mode=display">
\frac{1}{n} \sum (x-\bar x)^2
</script></div>

<p>Covariance measure the correlation property between two series data</p>
<div class="mathblock"><script type="math/tex; mode=display">
\frac{1}{n} \sum (x-\bar x)(y-\bar y)
</script></div>

<p>Covariance matrix measure the variance and covariance in n series/dimension data. For example, we have a sample of data as below</p>

<!-- <label for='Table-ID4' class='margin-toggle'> &#8853;</label><input type='checkbox' id='Table-ID4' class='margin-toggle'/><span class='marginnote'>Table 1: a simple statistic for demonstration. </span> -->

<table>
  <thead>
    <tr>
      <th style="text-align: center">Â </th>
      <th style="text-align: center"><strong>Height</strong></th>
      <th style="text-align: center"><strong>Weight</strong></th>
      <th><strong>Grade</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Boy1</td>
      <td style="text-align: center">170cm</td>
      <td style="text-align: center">50kg</td>
      <td>80</td>
    </tr>
    <tr>
      <td style="text-align: center">Boy2</td>
      <td style="text-align: center">â€¦</td>
      <td style="text-align: center">â€¦</td>
      <td>â€¦</td>
    </tr>
    <tr>
      <td style="text-align: center">Boy3</td>
      <td style="text-align: center">â€¦</td>
      <td style="text-align: center">â€¦</td>
      <td>â€¦</td>
    </tr>
  </tbody>
</table>

<p>We can use a matrix <script type="math/tex">A</script> to represent this data. 
Then the covariance matrix should be <script type="math/tex">A^T A</script>.</p>

<p><script type="math/tex">A^T A(0, 0), A^T A(1, 1), A^T A(2, 2)</script> is the variance in height, weight, grade dimension, respectively.</p>

<p><script type="math/tex">A^T A(0, 1)</script> is the covariance between height and weight dimensions. Other element in <script type="math/tex">A^T A</script> can be interpreted in the same way.</p>

<h2 id="the-solutions-of-linear-equations"><strong><em>The solutions of linear equations</em></strong></h2>
<p>Suppose we have linear equations:</p>
<div class="mathblock"><script type="math/tex; mode=display">
Ax = b  \quad \textsf{(Non-homogeneous linear equations)}\\
Ax = 0 \quad \textsf{(Homogeneous linear equations)}
</script></div>
<p>where <script type="math/tex">A</script> is <script type="math/tex">m</script> rows and <script type="math/tex">n</script> cols.</p>

<p>We need to analyze the rank of matrix and its augmented matrix to figure out whether there is solution or not.</p>

<p>First, we can decompose <script type="math/tex">Ax</script> to</p>
<div class="mathblock"><script type="math/tex; mode=display">
Ax = \sum_i^n x_iA_{col_i}
</script></div>
<p>It means the linear equations can be seen as linear combination of column vectors of matrix <script type="math/tex">A</script>.</p>

<h3 id="-non-homogeneous-linear-equations">* Non-homogeneous linear equations</h3>

<p>It is intuitive to see that if <script type="math/tex">R(A, b) > R(A)</script>, <script type="math/tex">b</script> is linearly independent with <script type="math/tex">A</script>.
It is impossible to find a vector combination to represent <script type="math/tex">b</script>. 
In other words, there is no solution. <label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote"><script type="math/tex">R(A)</script> means the rank of matrix A. </span></p>

<p>When <script type="math/tex">R(A, b) = R(A)</script>, we can know <script type="math/tex">b</script> is linearly dependent with <script type="math/tex">A</script>. 
So if <script type="math/tex">% <![CDATA[
R(A, b) = R(A) < n %]]></script>, <script type="math/tex">Ax=b</script> has infinite solutions; 
if <script type="math/tex">R(A, b) = R(A) = n</script>, the equations has only one solution.</p>

<h3 id="-homogeneous-linear-equations">* Homogeneous linear equations</h3>

<p>Homogeneous case is similar with non-homogeneous. 
Since <script type="math/tex">b=\boldsymbol{0}</script>, if <script type="math/tex">R(A) = n</script> (full ranked), there is only zero solution. 
If <script type="math/tex">A</script> is not full ranked, we can find a linear combination of colunm vectors to represent the linear dependent ones. Therefore, there are infinite solutions.</p>

<h2 id="matrix-decomposition"><strong><em>Matrix decomposition</em></strong></h2>
<h3 id="-lu-decomposition">* LU decomposition</h3>
<p>When matrix <script type="math/tex">A</script> is invertible, then we can decompose <script type="math/tex">A = LU</script>. We usually have two different composition:</p>
<ol>
  <li><script type="math/tex">L</script> is lower unit triangular matrix (diagonal elements are 1) and <script type="math/tex">U</script> is a upper triangular matrix;</li>
  <li><script type="math/tex">L</script> is lower triangular matrix and <script type="math/tex">U</script> is an upper unit triangular matrix</li>
</ol>

<h3 id="-cholesky-llt-decomposition">* Cholesky (LLT) decomposition</h3>
<p>When matrix <script type="math/tex">A</script> is a hermitian positive-definite matrix, then we can decompose <script type="math/tex">A=LL^T</script>, where <script type="math/tex">L</script> is lower triangular matrix
<label for="2," class="margin-toggle sidenote-number"></label><input type="checkbox" id="2," class="margin-toggle" /><span class="sidenote">The main application of matrix decomposition is to solve Ax=b linear system. Decomposition is significant faster than directly calculating inverse matrix.  </span></p>

<h3 id="-cholesky-variant-ldlt-decomposition">* Cholesky variant (LDLT) decomposition</h3>
<p>It is the same with cholesky, but we donâ€™t want to calculate the square root, which slow down the calculation and lose accuracy. 
So we can decompose <script type="math/tex">A=LDL^T</script> instead, where <script type="math/tex">L</script> is lower unit triangular matrix and <script type="math/tex">D</script> is diagonal matrix</p>

<p>(LLT and LDLT has been introduced in <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">HERE</a>)</p>

<h2 id="solving-linear-least-squares-systems"><strong><em>Solving linear least squares systems</em></strong></h2>
<p>There are three typical ways to solve linear least square system (SVD, QR factorization, Cholesky decomposition).</p>

<p>Check <a href="https://eigen.tuxfamily.org/dox/group__LeastSquares.html">here</a> for Eigen library code.</p>

<h3 id="-an-example">* An example</h3>
<p>Iâ€™d like to use a simple example to explain the Cholesky approach.</p>

<p>Suppose we have 12 sample points on the 3D plane of equation <script type="math/tex">z = 2x+3y</script> (with some noise). 
We can write these samples into a matrix.</p>
<div class="mathblock"><script type="math/tex; mode=display">
A = 
\begin{pmatrix}
x_1 & y_1 \\
x_2 & y_2 \\
... & ... \\
x_{12} & y_{12}
\end{pmatrix} 
\quad
b = 
\begin{pmatrix}
2x_1+3y_1+noise_1 \\
2x_2+3y_2+noise_2 \\
...  \\
2x_{12}+3y_{12}+noise_{12}
\end{pmatrix} 
</script></div>

<p>We want to calculate the coefficient <script type="math/tex">x=\begin{pmatrix} a\\ b \end{pmatrix}</script>, 
such that</p>
<div class="mathblock"><script type="math/tex; mode=display">
    \arg \min_x ||Ax-b||^2
</script></div>

<p>If we calculate the partial derivation w.r.t. <script type="math/tex">a,b</script>, and let the partial derivations equal to 0.
We will have below linear equation.</p>
<div class="mathblock"><script type="math/tex; mode=display">
    A^T Ax=A^T b
</script></div>
<p>The solution of above equation is equivalent to the optimal estimation a, b. (it is easy to derive and prove).</p>

<p>To solve <script type="math/tex">A^T Ax=A^Tb</script>, we can apply Cholesky decomposition, but if <script type="math/tex">A^TA</script> is ill-conditioned <label for="2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="2" class="margin-toggle" /><span class="sidenote">Check <a href="https://en.wikipedia.org/wiki/Condition_number">here</a> for more details about ill-condition  </span>
(e.g. <script type="math/tex">A^TA</script> is not invertible), it will lead to problems.</p>

<h2 id="pseudoinverse-matrix"><strong><em>Pseudoinverse matrix</em></strong></h2>
<p>Pseudoinverse matrix has two types:</p>
<ol>
  <li>Left pseudoinverse matrix.</li>
  <li>Right pseudo inverse matrix.</li>
</ol>

<p>For more details, check <a href="https://www.qiujiawei.com/linear-algebra-16/">HERE</a>.</p>

<h2 id="derivatives-of-scalar-vector-matrix">Derivatives of Scalar, Vector, Matrix</h2>
<p>Many times, we need to calculate the derivatives of (scalar, vector, matrix) w.r.t. (scalar, vector, matrix). 
Iâ€™d give a comprehensive summary about those calculations. You can see <a href="http://cs231n.stanford.edu/vecDerivs.pdf">here</a> for an intuitive introducation.</p>

<p><strong>Case 1: The derivative of scalar w.r.t. scalar</strong></p>

<p>This is the simplest case that you can apply the standard derivative rules.</p>

<p><br />
<strong>Case 2: The derivative of scalar w.r.t. vector and matrix</strong></p>

<p>The results are related with the transpose of corresponding coeffcients. <label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Check <a href="https://blog.csdn.net/acdreamers/article/details/44662633">here</a> for calculation rules. </span></p>

<p>For example, 
<script type="math/tex">y = x_1^T x_2</script> where <script type="math/tex">y</script> is a scalar, <script type="math/tex">x_1, x_2</script> are vectors. Then <script type="math/tex">\frac{\partial y}{\partial x_2} = x_1, \frac{\partial y}{\partial x_1} = x_2</script>.</p>

<p><script type="math/tex">y= x_1^T M x_2</script> where <script type="math/tex">M</script> is a matrix. In this case, you can consider the trace of matrix as below</p>
<div class="mathblock"><script type="math/tex; mode=display">
    \frac{\partial y}{\partial M} = \frac{\partial tr(x_1^T M x_2)}{\partial M} = \frac{\partial tr(x_2 x_1^T M )}{\partial M} = (x_2 x_1^T)^T = x_1 x_2^T
</script></div>

<p>Note that the second order derivative of scalar w.r.t. vector will be the standard Hessian matrix. (The first order derivative will give a vector result, the second order is to calculate the derivative of vector w.r.t. vector which gives a Jacobian matrix result, but people call it as Hessian) <label for="2," class="margin-toggle sidenote-number"></label><input type="checkbox" id="2," class="margin-toggle" /><span class="sidenote">See wiki <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a> and <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a>.  </span></p>

<p><br />
<strong>Case 3: The derivative of vector w.r.t. vector</strong></p>

<p>From this case, the above coefficient transpose cannot be applied. You need to explicitly expand vector to scalar representation, and then calculate derivatives.</p>

<p>For example, <script type="math/tex">y = Ax</script> where <script type="math/tex">x, y</script> are vectors and <script type="math/tex">A</script> is a matrix. Then <script type="math/tex">\frac{\partial y}{\partial x} = A</script>. <script type="math/tex">A</script> is also called Jacobian.</p>

<p><br />
<strong>Case 4: The derivative of vector w.r.t. matrix</strong></p>

<p>This case is little bit complicated. The result will be a hyper-matrix. Letâ€™s take <script type="math/tex">y = Ax</script> as an example, it looks like 
<script type="math/tex">\begin{pmatrix}
\frac{\partial y_1}{\partial A} \\
\frac{\partial y_2}{\partial A} \\
...  \\
\frac{\partial y_n}{\partial A}
\end{pmatrix}</script></p>

<p>The matrix element <script type="math/tex">\frac{\partial y_i}{\partial A}</script> is a matrix too (the same dimension with A).</p>

<p>Letâ€™s look at a simple <script type="math/tex">2 \times 2</script> example in below picture.</p>
<figure><figcaption></figcaption><img src="/Notes/assets/linear_algebra/marix_derivative.jpg" /></figure>

<p>From this example, we can have another conclusion:</p>

<p>If a function <script type="math/tex">f: R^{m\times n} \rightarrow  R^{p\times q}</script> which map <script type="math/tex">m\times n</script> input to <script type="math/tex">p \times q</script> output. Then the derivative of output w.r.t. input should also be able to map <script type="math/tex">R^{m\times n} \rightarrow  R^{p\times q}</script> (based on the Taylor expansion).</p>

<p><br />
<strong>Case 5: The derivative of matrix w.r.t. matrix</strong></p>

<p>This case is similar with case 4, but the dimension has increased.
Again, letâ€™s look at a <script type="math/tex">2 \times 2</script> example</p>
<figure><figcaption></figcaption><img src="/Notes/assets/linear_algebra/marix_derivative2.jpg" /></figure>

<p><strong>In one word, when we face case 4 or 5, the derivative calculation becomes complicated and there isnâ€™t a simple representation.</strong></p>




    </article>
    <span class="print-footer">Miscellaneous (linear algebra) - Canyu Le</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2019 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;CANYU LE &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2019</span> 
</div>  
</footer>

  </body>
</html>
