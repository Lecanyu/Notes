<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Miscellaneous</title>
  <meta name="description" content="Personal notes (memorandum).">


  <link rel="stylesheet" href="/Notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/Notes/machine_learning/miscellaneous/">
  <link rel="alternate" type="application/rss+xml" title="Notes" href="http://localhost:4000/Notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/Notes/">Contents</a>
		<a href="https://github.com/Lecanyu/Notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Miscellaneous</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>Some basic machine learning knowledge which are frequently applied in various research (also often asked in interview). I wrote them down for review.</p>

<h2 id="pareto-optimality">Pareto optimality</h2>
<p>It is a resource allocation state in which it is impossible that reallocate the resources so as to improve an individual situation without making other individuals worse off.</p>

<p>It is a minimum notion of efficiency but unnecessarily lead to desire results since it doesnâ€™t take fairness and equality into account.</p>

<h2 id="pareto-improvement">Pareto improvement</h2>
<p>It means we can reallocate the limited resources to make some individuals better off without making any other individuals worse off.</p>

<h2 id="precision-recall-estimation">Precision-recall estimation</h2>
<div class="mathblock"><script type="math/tex; mode=display">
Recall = \frac{TP}{TP+FN}, \quad
Precision = \frac{TP}{TP+FP}
</script></div>

<p>False positive (FP): classify the negative class into positive category.</p>

<p>False negative (FN): classify the positive class into negative category.</p>

<p>Recallè¶Šä½ï¼Œå¾ˆå¤šæ­£ä¾‹è¢«é”™è¯¯åˆ†ç±»ï¼ˆæ”¾èµ°äº†å¾ˆå¤šæ­£ä¾‹ï¼‰
Precisionè¶Šä½ï¼Œå¾ˆå¤šè´Ÿä¾‹è¢«é”™è¯¯åˆ†ç±»ï¼ˆåŒ…è¿›äº†å¾ˆå¤šè´Ÿä¾‹ï¼‰</p>

<h2 id="roc-and-auc">ROC and AUC</h2>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/AUC_ROC.png" /></figure>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/ROC_curve.png" /></figure>
<p>ROC is the curve based on TPR and FPR.</p>

<p>AUC is the area under the ROC curve.</p>

<p>AUCä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œå¯ä»¥æœ‰æ•ˆå¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ã€‚æ¯”å¦‚ï¼šåœ¨åƒåœ¾é‚®ä»¶åˆ†ç±»ä¸­ï¼Œåƒåœ¾é‚®ä»¶çš„label = 1ï¼Œåªå æ€»æ ·æœ¬çš„1%ï¼›éåƒåœ¾é‚®ä»¶çš„label = 0ï¼Œå æ€»æ ·æœ¬99%ã€‚
å¦‚æœä¸€ä¸ªnaiveåˆ†ç±»å™¨æŠŠæ‰€æœ‰çš„æ ·æœ¬åˆ†æˆ0ï¼Œé‚£ä¹ˆä¹Ÿæœ‰99%çš„ç²¾ç¡®åº¦ã€‚
ä½†æ˜¯å¦‚æœç”¨AUCä½œä¸ºè¯„ä»·æŒ‡æ ‡æ—¶ï¼Œå¯ä»¥å¾—åˆ°TPR=0ï¼ˆå®é™…æ˜¯åƒåœ¾é‚®ä»¶çš„ï¼Œåˆ†å¯¹äº†å¤šå°‘ï¼‰ï¼ŒFPR=0ï¼ˆå®é™…éåƒåœ¾é‚®ä»¶ï¼Œåˆ†é”™äº†å¤šå°‘ï¼‰ï¼Œä¸ç®¡åˆ†ç±»çš„é˜ˆå€¼æ€ä¹ˆå–ï¼Œå§‹ç»ˆéƒ½æ˜¯åæ ‡ç³»ä¸­çš„ç‚¹(0, 0)ï¼Œå› æ­¤AUC=0 ?
<span style="color:red"> (éœ€è¦è¿›ä¸€æ­¥ææ¸…æ¥šAUCçš„è®¡ç®—å…¬å¼)</span></p>

<h2 id="average-precision-ap-and-map">Average precision (AP and mAP)</h2>
<p>AP is a common metric in various object detection paper. 
It is related with recall and precision. 
Conceptually, it is the area of recall-precision curve. See below picture.</p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/AP.png" /></figure>

<p>But for convenience, people usually approximately estimate this area by interpolation. 
Check <a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">here</a> for the specific algorithm.</p>

<h2 id="intersection-over-union-iou">Intersection over union (IoU)</h2>
<p>This metric has been widely used on object detection. 
It is used for measuring how accurate the predicted bounding box is.</p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/IoU explanation.png" /></figure>

<h2 id="bias-and-variance">Bias and variance</h2>
<p>Bias is used to describe how close it is between the prediction and true value.</p>

<p>Variance is used to describe how concentrated the predictions are.</p>

<p>Overfitting usually lead to high variance since it is trained to fit the noises in training data.</p>

<p>Underfitting usually lead to high bias since it is unable to capture enough correlation between input and target.</p>

<h2 id="classification-and-regression">Classification and regression</h2>
<p>The training target in classification is discrete.</p>

<p>The training target in regression is continuous.</p>

<h2 id="supervised-learning-unsupervised-learning-semi-supervised-learning">Supervised learning, unsupervised learning, semi-supervised learning</h2>
<p>The training data is labeled in supervised learning like classification</p>

<p>The training data is unlabeled in unsupervised learning like clustering, generative adversarial network, EM algorithm, PCA and etc.</p>

<h2 id="off-policy-and-on-policy">Off-policy and on policy</h2>
<p>Off-policy is the training target of a policy is different from the behavior policy like ğœ€-greedy (more exploration)</p>

<p>On policy is the training target of a policy is exact the same with the behavior policy. (less exploration)</p>

<p>On policy can converge faster than off-policy.</p>

<h2 id="feature-selection-how-to-select-feature-from-massive-statistical-data">Feature selection (how to select feature from massive statistical data)</h2>
<p>Filtering: according to the feature variance</p>

<p>Wrapper: we can randomly pick some features to train and evaluate the result</p>

<p>Embedded: we can use a machine learning method to train first and then check how important those features are.</p>

<h2 id="feature-dimensionality-reduction">Feature dimensionality reduction</h2>
<p>There two common methods: PCA and LDA.
PCA is an unsupervised dimensionality reduction method, whereas LDA is supervised.</p>

<p>The main difference between them:</p>

<p>PCA try to extract new feature vector which has big variance.
LDA try to project original space to low dimensional space so as to maximize the distance between different classes and minimize the within-class variance.
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Please check <a href="https://www.cnblogs.com/pinard/p/6244265.html">here</a> for details about LDA.  </span></p>

<h2 id="generative-model">Generative model</h2>
<p>All types of generative models aims at learning the true distribution of training data so as to generate new data point with some variations. But it is not always possible to learn the exact data distribution.</p>

<p>It belongs to unsupervised learning (we donâ€™t need to label the training data).</p>

<p>Two types of generative models:</p>
<ul>
  <li>Variational autoencoder (VAE)</li>
  <li>Generative Adversarial Networks (GAN) 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">check <a href="https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a">here</a> and <a href="https://arxiv.org/pdf/1406.2661.pdf">paper</a> for detailed introducation.  </span></li>
  <li><em>The generator network and discriminator network (evaluator).</em></li>
</ul>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/generative_adversarial_network.png" /></figure>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/generative_adversarial_network_2.png" /></figure>

<h2 id="p-np-np-complete-np-hard">P, NP, NP-Complete, NP-hard</h2>
<p>P is the problem that we can find a solution which can be finished in polynomial time. 
<label for="mf-id-whatever" class="margin-toggle">âŠ•</label><input type="checkbox" id="mf-id-whatever" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/Notes/assets/machine_learning/P_NP.png" /><br />The relationship between P, NP, NP-hard and NP-Complete. </span></p>

<p>NP is the problem that we may not be able to find a polynomial time complexity solution but we can validate if a specific solution is correct or not within polynomial time.</p>

<p>NP-hard is a more general problem that some NP problems can reduce to within polynomial time. Once a NP-hard problem is solved, all such reducible NP problems will be solved. (Note: Sometimes, after NP reduce to NP-hard, this NP-hard problem may not be validated within polynomial time).
The global composition in jigsaw puzzle solving can be seen as a SAT problem variant, but it cannot be validated within polynomial time. So it belongs to NP-hard.</p>

<p>NP-complete is overlap between NP-hard and NP, which means they are reduced from NP and still can be validated within polynomial time.</p>

<p>SAT (satisfiability) problem: if we can find a Boolean assignment that make a system output true. For example, we cannot find such assignment for  <script type="math/tex">\neg p \land p</script>, but we can find for <script type="math/tex">p \land q</script>.</p>

<p>Variant: 2-SAT and 3-SAT. See this intuitive <a href="https://www.zhihu.com/question/55516280/answer/145138234">example</a> (è¿‡å¹´äº†ï¼Œæ­£æ‰“ç®—çƒ§å¹´å¤œé¥­ blablaâ€¦)</p>

<p>Problem Reduce: this means we can convert a problem into a more general (usually more difficult) problem. For example, problem A: find the minimum element in an array. Problem B: sort the whole array. We can say A can be reduce to B, since if we can solve the problem B, problem A will be solved trivially.</p>

<h2 id="the-advantage-of-max-pooling-layer">The advantage of max pooling layer</h2>
<p>Reduce the number of parameters. To avoid over-fitting</p>

<p>Increase the perceptual field</p>

<p>Extract the main features.</p>

<h2 id="convolutional-neural-network">Convolutional Neural Network</h2>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/CNN_filter_calc.png" /></figure>

<h2 id="batch-normalization">Batch normalization</h2>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/batch_normalization.png" /></figure>

<p>Suppose input= [batch, height, width, depth]. 
If we use axes= [0,1,2] to calculate (e.g. mean, var = tf.nn.moments(input, axes=[0, 1, 2])), then the output of mean, var will be a 1-D vector with size=depth. 
If we use axes=[0] to calculate, then the output of mean, var will be a 3-D vector with size=[height, width, depth]. The picture below demonstrates this 3-D vector case.</p>

<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/batch_normalization_ex.png" /></figure>

<h2 id="group-normalization">Group normalization</h2>
<p>The comparison between several normalization methods</p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/group_normalization.png" /></figure>

<p>Batch normalization has several defects:</p>

<ol>
  <li>BN only works fine with large batch size, but this cannot guaranteed on complex tasks (e.g. object detection) due to insufficient memory.</li>
  <li>BN use mini-batchâ€™s mean and variance to normalize during training, but use moving-average (æ»‘åŠ¨å¹³å‡) mean and variance to normalize during testing. It introduces inconsistency, especially when data distribution between training and testing are different.</li>
</ol>

<p>Unlike BN to normalize on dimension [batch, width, height], 
the group normalization (GN) try to normalize on dimension [width, height, channel=k] (k is a hyperparameter). It doesnâ€™t matter with batch. So it solves the first defect.
Moreover, GN always use group mean and variance during training and testing. <label for="10," class="margin-toggle sidenote-number"></label><input type="checkbox" id="10," class="margin-toggle" /><span class="sidenote">IN, LN and GN doesnâ€™t use batch dimension to normalize. </span></p>

<h2 id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h2>
<p>An tutorial and introduction about <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">RNN</a></p>

<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/RNN.png" /></figure>

<h2 id="hidden-markov-model">Hidden Markov Model</h2>
<p>An specific dice <a href="https://www.zhihu.com/question/20962240">example</a> will help to understand.</p>

<p>The basic elements:
Transition matrix, emission matrix, initial state.</p>

<p>Three basic problems:</p>
<ol>
  <li>Given the HMM model, how to calculate the probability of an observation. (forward algorithm)</li>
  <li>Given the HMM model and a sequence of observations, how to estimate which the most possible hidden state sequences are. (Viterbi algorithm, dynamic programming)</li>
  <li>Given the observation data, how to estimate the model parameters. (learning problem, Baumâ€“Welch algorithm)</li>
</ol>

<p>Note:
In the learning problem, 
the target of forward algorithm is to calculate the <script type="math/tex">\alpha_i(t) = P(X_{t=i}, y_1,y_2, ..., y_t)</script>.
The target of backward algorithm is to calculate the <script type="math/tex">\beta_i(t) = P(y_{t+1}, y_{t+2}, ..., y_T|X_{t=i})</script>.
The target of forward-backward algorithm is to calculate the <script type="math/tex">\gamma_i(t) = P(X_{t=i}|y_1, y_2, ..., y_T)</script>.</p>

<p>For the details, please check <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">here</a></p>

<h2 id="em-expectation-maximization-algorithm">EM (Expectation-Maximization) algorithm</h2>
<p>An intuitive <a href="https://www.youtube.com/watch?v=REypj2sy_5U">tutorial</a>.</p>

<p>EM algorithm is an unsupervised learning.</p>

<h2 id="maximum-likelihood-vs-maximum-a-posterior-map">Maximum likelihood vs Maximum A Posterior (MAP)</h2>
<p><script type="math/tex">P(X|Y) = \frac{P(Y|X) * P(X)}{P(Y)}</script> &lt;=&gt; <script type="math/tex">Posterior = \frac{Likelihood * Prior}{Evidence}</script>.</p>

<p>Usually given the training data <script type="math/tex">D</script>, our target is to maximize the posterior <script type="math/tex">P(\theta|D)</script>, where <script type="math/tex">\theta</script> is the model parameters. 
To do that, we usually have two approaches:</p>

<ol>
  <li>
    <p>Maximum likelihood.
Since <script type="math/tex">P(\theta|D)=\frac{P(D|\theta)P(\theta)}{P(D)}</script>, if we can assume the prior is a constant, then <script type="math/tex">\max P(\theta|D) = \max P(D|\theta)</script>.
Maximum likelihood estimation is widely used in solving various machine learning models, especially when prior is unknown.</p>
  </li>
  <li>
    <p>Maximum A Posterior (MAP).
Sometimes, we may not be able to make such assumption that the prior is a constant. Instead, prior may satisfy a distribution. In this case, the target is  <script type="math/tex">P(\theta|D) =  \mathop{\arg\min}_{\theta} P(D|\theta)P(\theta)</script>. Since every data is generated independently, we have <script type="math/tex">P(\theta|D) =  \mathop{\arg\min}_{\theta} \Pi_i^n P(D=x_i|\theta)P(\theta)</script>.
If we use prior <script type="math/tex">P(\theta)</script> to represent the complexity of a model, then MAP is linked with structure risk minimization.</p>
  </li>
</ol>

<h2 id="a-systematic-probabilistic-graph-model-course">A systematic probabilistic graph model course</h2>
<p>https://ermongroup.github.io/cs228-notes/</p>

<h2 id="bayes-rule-and-bayes-network">Bayes rule and Bayes network</h2>
<p>Bayes rule is widely used on various machine learning problems. Some basic math/probabilistic knowledge need to be clarified.</p>

<p>The general Bayes rule for joint distribution:</p>
<div class="mathblock"><script type="math/tex; mode=display">
P(x_1, x_2, ..., x_n) = P(x_1|x_2, ...,x_n)*P(x_2|x_3, ..., x_n)* ... * P(x_{n-1}|x_{n}) * P(x_{n})
</script></div>

<p>If we have a bayes network which models the problem, we can simplify the calculation. For example, given a simple network as below, we have</p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/bayes_net1.png" /></figure>
<div class="mathblock"><script type="math/tex; mode=display">
P(A, B, C) = P(A)*P(B|A)*P(C|B)
</script></div>
<p>This can work because <span>â€‹<script type="math/tex"> P(C|B) = P(C|A, B) </script></span> (when we know variable B, the variable A and C can be seen independent. But if B is unknown, A and C are dependent. This is because we can use the medium variable B to represent the dependence between A and C.)</p>

<p>From this network, we can have some other property like</p>
<div class="mathblock"><script type="math/tex; mode=display"> P(C|A) = \int P(C|B)P(B|A) \mathrm{d}B </script></div>
<p>Because <span>â€‹<script type="math/tex"> P(C|B) = P(C|A, B), P(C|B)P(B|A) = P(C|A, B)*P(B|A) = \frac{P(A, B, C)}{P(A)} </script></span> and <span>â€‹<script type="math/tex"> \int P(A,B,C) \mathrm{d}B = P(A,C) </script></span></p>

<p>Generally, we also have below property without graph structure if <span>â€‹<script type="math/tex">A, B</script></span> are independent.</p>
<div class="mathblock"><script type="math/tex; mode=display"> P(A, B|C) = P(A|C)P(B|C) </script></div>
<p>Because <span>â€‹<script type="math/tex">P(A, B, C) = P(A, B|C)P(C) = P(A|B, C)P(B|C)P(C) = P(A|C)P(B|C)P(C)</script></span></p>

<h2 id="several-concepts-need-to-be-distinguished">Several concepts need to be distinguished</h2>
<ol>
  <li>Bayesian Network</li>
  <li>Markov Random Field</li>
  <li>Conditional Random Field</li>
  <li>Markov Chain</li>
  <li>Hidden Markov Model</li>
  <li>Markov Decision Process</li>
</ol>

<h2 id="why-divide-n-1-to-get-unbiased-variance-estimation-in-sampling-data">Why divide n-1 to get unbiased variance estimation in sampling data</h2>
<p>http://blog.sina.com.cn/s/blog_c96053d60101n24f.html</p>

<h2 id="a-good-systematic-chinese-tutorial-for-machine-learning">A good systematic Chinese tutorial for machine learning</h2>
<p>https://nndl.github.io/</p>

<p>The summary of <a href="https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf">math</a> part is a good material for reviewing the math background.</p>

<h2 id="information-theory">Information theory</h2>
<p>Refer to <a href="https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf">here</a> for detailed introducation.</p>

<p>Encode length for a random variable <script type="math/tex">X=x</script>, <script type="math/tex">I(x)= -\log p(x)</script>.</p>

<p>Entropy: the average length for optimal encoding the whole of random variable X.  <script type="math/tex">H(x) = -\sum_x p(x) \log p(x)</script>. 
The more stochastic variable is, the larger entropy is.</p>

<div class="mathblock"><script type="math/tex; mode=display">
\mbox{Joint entropy:  } H(x, y) \\
\mbox{Conditional entropy:  } H(x|y) \\
H(x|y) = H(x,y) - H(y)
</script></div>

<p>Cross-entropy: the average encoding length when we use distribution <script type="math/tex">q</script> to encode information <script type="math/tex">x</script> whose real distribution is <script type="math/tex">p</script>.</p>
<div class="mathblock"><script type="math/tex; mode=display">
H(p,q)= -\sum_x p(x) \log q(x)
</script></div>
<p>Obviously, <script type="math/tex">H(p,q)=H(p)</script>, when <script type="math/tex">p(x)=q(x)</script>. We have minimum <script type="math/tex">H(p,q)</script>, when <script type="math/tex">p(x)=q(x)</script>.</p>

<p>Kullback-Leibler divergence (KL-divergence):</p>
<div class="mathblock"><script type="math/tex; mode=display">
H(p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = H(p,q) - H(p) >= 0
</script></div>
<p>The meaning of KL-divergence is very similar with cross-entropy. 
Both of them measure how different between distribution <script type="math/tex">p(x)</script> and <script type="math/tex">q(x)</script>. So when <script type="math/tex">p(x)</script> is given, the optimization process of <script type="math/tex">H(p||q)</script> and <script type="math/tex">H(p,q)</script> is the same.
Comparing with cross-entropy, KL-divergence measures the absolute difference. When <script type="math/tex">p(x)=q(x)</script>, <script type="math/tex">H(p||q)=0</script>.</p>

<h2 id="logistic-regression">Logistic regression</h2>
<p>sigmoid function <script type="math/tex">f(x; w)=\frac{1}{1+e^{-wx}}</script>: convert linear classification result to a probability.</p>

<p>maximize likelihood =&gt; solve the parameters in linear classification.</p>

<p>The key here is that sigmoid function normalize raw result into 0 and 1. Then we can construct below likelihood.</p>

<div class="mathblock"><script type="math/tex; mode=display">
\max P(w|Y) <=> \max P(Y|X, w) = \Pi_{i=1}^n f^{y_i}(x_i; w)(1-f(x_i;w))^{1-y_i}
</script></div>

<p>Apply logarithm on both side.</p>
<div class="mathblock"><script type="math/tex; mode=display">
\log P(y|x, w) = \sum_{i=1}^n y_i\log f(x_i;w) + (1-y_i)\log (1-f(x_i; w))
</script></div>

<p>This is equivalent to cross-entropy. And then we can apply gradient descend to optimization.</p>

<p>Check <a href="https://tech.meituan.com/intro_to_logistic_regression.html">here</a> for details.</p>

<h2 id="support-vector-machine">Support Vector Machine</h2>
<p>Please check <a href="https://blog.csdn.net/u014433413/article/details/78427574">here1</a> and <a href="https://zhuanlan.zhihu.com/p/31652569">here2</a> for the details about naive SVM, SVM with kernel and SVM with soft margin.</p>

<p>Here, we give a brief introducation.</p>

<p>In SVM, we want to find a hyperplane <script type="math/tex">wx+b = 0</script> (w, b are the parameters to learn), so that this hyperplane can correctly divide two classes (the labels are -1, 1) data points.</p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/SVM1.png" /></figure>
<p>We define that the data points which are the nearest ones to hyperplane <script type="math/tex">wx+b=0</script> should locate on <script type="math/tex">wx+b=1</script> (for positive datapoint) and <script type="math/tex">wx+b=-1</script> (for negative datapoint). The reason why this definition makes sense is that we can always adjust the learned hyperplane by multiple a factor (e.g. k) without change the hyperplane in space 
(i.e. <script type="math/tex">wx+b=0</script> &lt;=&gt; <script type="math/tex">k(wx+b)=0</script>).</p>

<p>So our goal is to maximize the distance between <script type="math/tex">wx+b=1</script> and <script type="math/tex">wx+b=-1</script>. Obviously, the distance is 
<script type="math/tex">\frac{2}{||w||}</script>. We can write the objective function as below</p>
<div class="mathblock"><script type="math/tex; mode=display">
\min \frac{1}{2} w^T w \\
s.t. \sum_i y_i(wx_i+b) \ge 1
</script></div>
<p>To solve this objective, we apply Lagrangian multipler to convert original constrained problem to</p>
<div class="mathblock"><script type="math/tex; mode=display">
\max_{\lambda} \min_w \frac{1}{2} w^T w - \sum_i \lambda_i [y_i(wx_i+b)-1] \\
s.t. \lambda \ge 0
</script></div>
<p>Then calculate the partial gradients w.r.t. <script type="math/tex">w, b</script> first, then <script type="math/tex">\lambda</script>. <label for="2," class="margin-toggle sidenote-number"></label><input type="checkbox" id="2," class="margin-toggle" /><span class="sidenote">This is the naive SVM </span></p>

<p>Sometimes, the datasets cannot be divided by linear plane. So we usually use kernel tricks, which introduce nonlinear property in classifier. 
The main idea of kernel trick is to find a mapping <script type="math/tex">\phi(x)</script> to map original space to a higher space. The picture below gives an example. <label for="3," class="margin-toggle sidenote-number"></label><input type="checkbox" id="3," class="margin-toggle" /><span class="sidenote">This is the SVM with kernel. Check <a href="https://www.zhihu.com/question/24627666">here</a> for the reasons why kernel can map to a higher dimensions.  </span></p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/SVM_kernal.png" /></figure>
<p>The P mapping in above picture is the <script type="math/tex">\phi(x)</script>. You should distinguish the kernel function and dimensional mapping function.</p>

<p>However, sometimes, kernel can still fail. So people introduce the soft margin, which allows misclassification on some data points.
The objective is <label for="4," class="margin-toggle sidenote-number"></label><input type="checkbox" id="4," class="margin-toggle" /><span class="sidenote">This is soft margin. </span></p>
<div class="mathblock"><script type="math/tex; mode=display">
\min \frac{1}{2} w^T w + c\sum_i \xi_i \\
s.t. \sum_i y_i(wx_i+b) \ge 1 - \xi_i \\
\xi_i \ge 0
</script></div>
<p>The <script type="math/tex">\xi_i</script> can be seen as hinge loss: <script type="math/tex">\xi_i = \max (0, 1-y_i(wx_i+b))</script>. And SVM can be explained in hinge loss.</p>
<div class="mathblock"><script type="math/tex; mode=display">
\min \frac{1}{m}\sum_i^m \max (0, 1-y_i(wx_i+b)) + k||w||^2
</script></div>

<h2 id="kalman-filter">Kalman filter</h2>
<p>It is a algorithm for accurately estimating or predicting based on multiple observed data. 
See <a href="https://www.zhihu.com/question/22422121">here</a> for an intuitive explanation and example.</p>

<p>A classic example is SLAM in which we have multiple data collecting sensors like odometry, IMU and visual features. The Kalman filter is to solve how to reliably combine all sensor data and estimate a accurate pose.</p>

<p><strong>In linear case, the Kalman filter problem can be formulated as</strong></p>
<div class="mathblock"><script type="math/tex; mode=display">
x_k = A_k x_{k-1} + u_k + w_k\\
z_k = C_k x_{k} + v_k
</script></div>
<p>The first equation is motion equation where <script type="math/tex">x_k</script> is the state at <script type="math/tex">k</script>-th moment. <script type="math/tex">u_k</script> is motion measurement with noise <script type="math/tex">w_k</script> which is satisfied a gauss distribution <script type="math/tex">w_k \sim N(0, R_k)</script>.</p>

<p>The second equation is observation equation where <script type="math/tex">z_k</script> is the observation measurement with noise <script type="math/tex">v_k</script> which is also satisfied a gauss distribution <script type="math/tex">v_k \sim N(0, Q_k)</script>.</p>

<p><em>Since the motion and observation measurements are noisy (subject to gauss distribution) which means that the current state estimation is noisy too, our goal now is to find an optimal state estimation which has the minimum uncertainty (i.e. minimum covariance).</em></p>

<p>The way is to calculate Kalman Gain which is the optimal weight between motion and observation. <label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">I ignore the derivation of Kalman Gain, since it is a little bit complicated. You can search online about this. </span></p>

<p>The complete algorithm is:</p>

<p>First, predict currect state from previous state based on motion equation. 
(The previous state can be represented by gauss distribution <script type="math/tex">N(\hat x_{k-1}, \hat P_{k-1})</script>)</p>

<div class="mathblock"><script type="math/tex; mode=display">
\bar x_k = A_k \hat x_{k-1} + u_k \\
\bar P_{k} = A_k \hat P_{k-1} A_{k}^T + R 
</script></div>
<p>This equation is easy to understand according to the properties of gauss distribution (The new mean and new covariance from two gauss distribution).</p>

<p>Second, calculate the Kalman Gain</p>
<div class="mathblock"><script type="math/tex; mode=display">
K = \bar P_k C_k^T (C_k \bar P_k C_k^T + Q_k)^{-1}
</script></div>

<p>Finally, correct/modify the rough estimation from motion equation.</p>
<div class="mathblock"><script type="math/tex; mode=display">
\hat x_k = \bar x_k + K (z_k - C_k \bar x_k) \\ 
\hat P_k = (I - KC_k) \bar P_k
</script></div>

<p>If you derivate a little, you can find that if <script type="math/tex">Q_k = 0</script> (no covariance in observation), then <script type="math/tex">\hat x_k = C_k^{-1} z_k</script>, which means the final estimation is totally depended on observation equation.  There is a similar conclusion when <script type="math/tex">R_k = 0</script>.</p>

<p>Now we know the idea of Kalman filter in linear system.</p>

<p>In reality, the system is usually nonlinear (e.g. SLAM problem). To apply above idea, we need to extend KF to nonlinear case. The Taylor expansion can do this.</p>

<h2 id="cross-entropy-instead-of-mean-square-error-mse-as-the-loss-function-in-classification">Cross-entropy instead of mean square error (MSE) as the loss function in classification?</h2>
<p>when we do classification, we usually apply softmax (this is an important premise) to normalize the output value to 0-1. 
In this case, the gradient of MSE loss will be prone to 0 when the prediction is closed to 0 or 1. This will lead to slow convergence.
On the contrary, the gradient of cross-entropy is linear with the prediction changing. When the prediction is closed to the label, the gradient will be small and vice versa.</p>

<h3 id="-cross-entropy">* Cross-entropy</h3>
<div class="mathblock"><script type="math/tex; mode=display">
p(x_i) = softmax(x_i) = \frac{e^{x_i}}{\sum_i e^{x_i}} \\
f(x) = -\sum_i y_i \log p(x_i)
</script></div>
<p>where <script type="math/tex">f(x)</script> is the objective cross-entropy function. To minimize it, we calculate the gradient w.r.t. <script type="math/tex">x_i</script></p>

<div class="mathblock"><script type="math/tex; mode=display"> 
\frac{\partial f(x)}{\partial x_i} = - \frac{y_i}{p(x_i)}p^{'}(x_i) \\ 
p^{'}(x_i) = \frac{\partial p(x_i)}{\partial x_i} = p(x_i) - p^2(x_i)
</script></div>
<p>So we have</p>
<div class="mathblock"><script type="math/tex; mode=display">
\frac{\partial f(x)}{\partial x_i} = - \frac{y_i}{p(x_i)}p^{'}(x_i) = -y_i (1-p(x_i))
</script></div>
<p>When <script type="math/tex">p(x_i)</script> is closed to 1, the gradient will be prone to 0. (Remember that we use one-hot encoding to represent <script type="math/tex">\mathbf{y}, \mathbf{x}</script>)</p>

<h3 id="-mse">* MSE</h3>
<div class="mathblock"><script type="math/tex; mode=display">
f(x) = \sum_i (y_i - p(x_i))^2 \\ 
\frac{\partial f(x)}{\partial x_i} = 2(y_i - p(x_i))(-p^{'}(x_i)) = -2(y_i - p(x_i))(p(x_i) - p^2(x_i))
</script></div>
<p>So we have the gradient</p>
<div class="mathblock"><script type="math/tex; mode=display">
\frac{\partial f(x)}{\partial x_i} = -2p(x_i)(1-p(x_i))(y_i - p(x_i))
</script></div>
<p>When <script type="math/tex">p(x_i)</script> is closed to 0 and 1, the gradient will be closed to 0. This is not desirable and will slow down the learning process. Because if <script type="math/tex">y_i = 1, p(x_i) = 0</script>, we hope the learning step is large, but the gradient is 0.</p>

<h2 id="the-property-of-softmax">The property of softmax</h2>
<ol>
  <li>property: the big value will take a large portion of the probability.</li>
  <li>potential drawback: exponential problem.</li>
</ol>

<h2 id="covariance-and-correlation-coefficients">Covariance and correlation coefficients</h2>
<p>Covariance is a measure of how two variables change together, but its magnitude is unbounded, so it is difficult to interpret. By dividing covariance by the product of the two standard deviations, one can calculate the normalized version of the statistic. This is the correlation coefficient.</p>
<ol>
  <li>Covariance: represent the unnormalized correlation.</li>
  <li>Correlation coefficient: represent the normalized correlation.</li>
</ol>
<div class="mathblock"><script type="math/tex; mode=display">
\rho_{xy} = \frac{Cov(x, y)}{\sigma_x \sigma_y}
</script></div>

<p>Refer to <a href="https://www.investopedia.com/terms/c/correlationcoefficient.asp">here</a> for details.</p>

<h2 id="normal-distribution-and-multivariate-normal-distribution">Normal distribution and multivariate normal distribution</h2>
<ul>
  <li>One-variable</li>
</ul>
<div class="mathblock"><script type="math/tex; mode=display">
f(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
</script></div>

<ul>
  <li>Multi-variables</li>
</ul>
<div class="mathblock"><script type="math/tex; mode=display">
f(X=x_1,...,x_n) = \frac{1}{\sqrt{(2\pi)^n |C(X)|}} e^{-\frac{1}{2}(X-\bar X)^T C^{-1}(X)(X-\bar X)}
</script></div>
<p>where <script type="math/tex">|C(X)|</script> is the determinant of covariance matrix C(X). By the way, C(X) is usually denoted as <script type="math/tex">\mathbf{\Sigma}</script> in many literatures.</p>

<div class="mathblock"><script type="math/tex; mode=display">
X = \left[x_1, x_2, ..., x_n \right]^T \\

\bar X = mean \\

C(X) = 
\begin{pmatrix}
cov(x_1, x_1) & cov(x_1, x_2) & ... & cov(x_{n-1}, x_{n}) \\
 & ... & &  \\
cov(x_n, x_1) & cov(x_n, x_2) & ... & cov(x_{n}, x_{n})  \\
\end{pmatrix} 
</script></div>

<p>Refer to <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">here</a> for details.</p>

<h2 id="some-important-concepts-in-machine-learning-statistic-learning">Some important concepts in machine learning (statistic learning)</h2>
<p><strong>1. Hypothesis space</strong></p>

<p>Hypothesis space is a set of models which map the input to output.
When hypothesis space is given, the range of learning space is determined.
For example, if we plan to use a neural network as the model, the hypothesis space consists of all possible parameters of this neural network.</p>

<p><strong>2. Expected risk, Empirical risk, Structure risk</strong></p>

<p>In machine learning, we usually suppose all data are i.i.d (independently drawn from identical distribution).
The expected risk is defined as</p>
<div class="mathblock"><script type="math/tex; mode=display">
R_{exp}(f) = E(L(Y, f(X))) = \int_{X, Y} L(y, f(x))P(x, y)dxdy
</script></div>
<p>where <script type="math/tex">(X,Y)</script> are training data. <script type="math/tex">L(\cdot, \cdot)</script> is loss function. <script type="math/tex">P(X, Y)</script> is joint probability distribution.
The ideal learning procedure is to find a best <script type="math/tex">f</script> which can minimize <script type="math/tex">R_{exp}(f)</script>.
However, it is impossible to know joint probability distribution <script type="math/tex">P(X, Y)</script> beforehand.
Because there is no need to learn if we already know <script type="math/tex">P(X, Y)</script>. 
<label for="1," class="margin-toggle sidenote-number"></label><input type="checkbox" id="1," class="margin-toggle" /><span class="sidenote">Therefore, machine learning is usually an ill-posed problem. About the ill-posed problem, you can check <a href="https://en.wikipedia.org/wiki/Well-posed_problem">here</a> and <a href="https://stats.stackexchange.com/questions/433692/why-is-pattern-recognition-often-defined-as-an-ill-posed-problem">here</a> </span></p>

<p>In practice, we minimize empirical risk instead of expected risk.</p>
<div class="mathblock"><script type="math/tex; mode=display">
R_{emp}(f) = \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i))
</script></div>
<p>Obviously, <script type="math/tex">R_{exp}(f)</script> is the loss w.r.t. joint probability distribution which is absolutely accurate.
<script type="math/tex">R_{emp}(f)</script> is the loss w.r.t. the average of training data which is an approximation.
According to law of large numbers, this approximation could be accurate when giving enough training data.</p>

<p>Structure risk is similar with empirical risk. The only difference is that it introduces regularization term to penalize complex model to relieve overfitting.</p>
<div class="mathblock"><script type="math/tex; mode=display">
R_{str}(f) = \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
</script></div>

<p><strong>3. Cross validation, k-fold cross validation</strong></p>

<p>Cross validation is a strategy to select a model with best generalization ability.
When we have enough labeled data, we can simply split them into training, validation, testing sets and use validation set to select model. 
K-fold cross validation is also a common strategy when data are relatively insufficient.</p>

<p>The general procedure is as follows:</p>
<ol>
  <li>Shuffle the dataset randomly.</li>
  <li>Split the dataset into k groups</li>
  <li>For each unique group:
    <ol>
      <li>Take the group as a hold out or test data set</li>
      <li>Take the remaining groups as a training data set</li>
      <li>Fit a model on the training set and evaluate it on the test set</li>
      <li>Retain the evaluation score and model</li>
    </ol>
  </li>
  <li>Select a model with best score</li>
</ol>

<p><strong>4. Generative model, Discriminative model</strong></p>

<p>If a model try to learn joint probability distribution <script type="math/tex">P(X, Y)</script>, it is a generative model.
If a model learn conditional probability distribution <script type="math/tex">P(Y|X)</script> (i.e. decision boundary), it is a discriminative model.</p>

<p>These two concepts here are different from they are in GAN (generative adversarial network).</p>



    </article>
    <span class="print-footer">Miscellaneous - Canyu Le</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2020 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;CANYU LE &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2020</span> 
</div>  
</footer>

  </body>
</html>
