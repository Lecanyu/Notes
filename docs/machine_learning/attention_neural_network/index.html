<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Attention neural network</title>
  <meta name="description" content="Personal notes (memorandum).">


  <link rel="stylesheet" href="/Notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/Notes/machine_learning/attention_neural_network/">
  <link rel="alternate" type="application/rss+xml" title="Notes" href="http://localhost:4000/Notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/Notes/">Contents</a>
		<a href="https://lecanyu.github.io/">Resume</a>
		<a href="https://github.com/Lecanyu/Notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Attention neural network</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<h2 id="motivation">Motivation</h2>

<p>When input data are massive and noisy, it may not be a good idea to directly train a model from the whole of original data. Because it is difficult for models to capture the meaningful information behind the massive data.</p>

<p>For example, in my previous work on jigsaw puzzle solving, it is important to transfer the calculation to the stiching area instead of the whole fragment. In NLP field, it is unnecessary to seize all context to translate a few of local words.</p>

<p>Generally, human’s perceptual system also focus on some particular areas to obtain information.</p>

<h2 id="soft-selection-and-hard-selection">Soft selection and hard selection</h2>

<p>Researchers have realized the importance of attention, and they have proposed two approaches to fulfill attention mechanism.</p>

<ol>
  <li>
    <p>Soft selection.</p>

    <p>The selection (attention transferring) layer is differentiable, and thus the whole networks can be trained end to end.</p>
  </li>
  <li>
    <p>Hard selection</p>

    <p>The selection layer is not differentiable. A typical implementation of this layer is  reinforcement learning.</p>
  </li>
</ol>

<p>Here are the representive network structures for these two type selections 
<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">A nice explaination about attention network can be found <a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/">HERE</a> </span></p>

<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/attention_network1.png" /></figure>
<p>Left picture: soft selection, right picture: hard selection, the random choice can be learned by a reinforcement learning.</p>

<p>In image captioning, the complete network structure can be below picture.</p>
<figure><figcaption></figcaption><img src="/Notes/assets/machine_learning/attention_network_global.png" /></figure>

<p>The attention model (purple blocks) is the selection layer. <script type="math/tex">h_1, h_2, ..., h_{k-1}</script> is the input <script type="math/tex">c</script> in above two pictures.</p>

<p>The intuition is that the attention model picks some input from feature map vector <script type="math/tex">y_1, y_2, ..., y_n</script> (because softmax is easily dominated by the maximum one).</p>

<p>If you have difficult to understand, check <a href="https://blog.heuritech.com/2016/01/20/attention-mechanism/">HERE</a>.</p>

<h2 id="some-insights-about-structured-attention-networks">Some insights about Structured Attention Networks</h2>

<p>Here I’d like to tell some insights about the paper “Structured Attention Networks”. <label for="2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="2" class="margin-toggle" /><span class="sidenote">Kim, Yoon, et al. <a href="https://arxiv.org/pdf/1702.00887.pdf">Structured attention networks</a>. ICLR 2017 </span></p>

<p>The key contribution in this paper is that the authors use a CRF to model the attention layer.</p>

<p><strong><em>Now my understanding may be wrong. I need to further read and double check.</em></strong></p>




    </article>
    <span class="print-footer">Attention neural network - Canyu Le</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;CANYU LE &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
