<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Reinforcement learning book review</title>
  <meta name="description" content="Personal notes (memorandum).">


  <link rel="stylesheet" href="/Notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/Notes/machine_learning/reinforcement_learning_book/">
  <link rel="alternate" type="application/rss+xml" title="Notes" href="http://localhost:4000/Notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/Notes/">Contents</a>
		<a href="https://github.com/Lecanyu/Notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Reinforcement learning book review</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>I’ll organize my notes in the same way with book “Reinforcement Learning: An Introducation” by Richard S. Sutton and Andrew G. Barto, Nov 5, 2017.</p>

<p>These notes are written for helping me quickly review relevant knowledge and some important insights.</p>

<h2 id="introduction">Introduction</h2>

<p>This chapter primarily introduce some basic concepts and history about RL.</p>

<h2 id="part-1-tabular-solution-methods">Part 1, Tabular Solution Methods</h2>

<ol>
  <li>
    <p><a href="part1_muti_armed_bandits">Multi-armed Bandits</a></p>

    <p>This chapter mainly discuss the different methods for exploiting and explorating. It uses multi-armed bandits as a concrete example to compare the performance of those methods.</p>

    <p>Besides, it introduces a convergence condition about the action-value update.</p>

    <p>(Heads up: since in this multi-armed bandits example the decision is made based on the previous rewards, its action-value function is formulated from past rewards instead of future).</p>
  </li>
  <li>
    <p><a href="part1_finite_MDP">Finite Markov Decision Processes</a></p>

    <p>This chapter gives the formal definition of RL, such as MDP, action-value function, state-value function, policy.</p>

    <p>Moreover, it gives the Bellman equation to describe state-value and action-value including the optimal formulations as below.</p>

    <div class="mathblock"><script type="math/tex; mode=display">
  v_*(s) = \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})| S_t=s, A_t=a] \\
  = \max_a \sum_{s^{'}, r} p(s^{'}, r|s, a)[r+\gamma v_*(s^{'})] \\ 

  q_*(s,a) = \mathbb{E}[R_{t+1}+\gamma \max_{a^{'}} q_*(S_{t+1}, a^{'})| S_t=s, A_t=a] \\
  = \sum_{s^{'}, r}p(s^{'}, r|s, a)[r+\gamma \max_{a^{'}}q_*(s^{'}, a^{'})]
  </script></div>

    <p>(In this chapter, it focus on future rewards with a discount ratio.)</p>
  </li>
  <li>
    <p><a href="part1_DP">Dynamic Programming</a></p>

    <p>This chapter introduces how to find optimal policy by dynamic programming. This method only works when the environment is well-defined (i.e. model-based).</p>
  </li>
</ol>



    </article>
    <span class="print-footer">Reinforcement learning book review - Canyu Le</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2020 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;CANYU LE &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2020</span> 
</div>  
</footer>

  </body>
</html>
